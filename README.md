# ML1
# Метрические алгоритмы классификации
**Метрические методы обучения** -- методы, основанные на анализе сходства объектов.

**_Мерой близости_** называют функцию расстояния ![](http://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%3A%20%28X%20%5Ctimes%20X%29%20%5Crightarrow%20%5Cmathbb%7BR%7D). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга.

Метрические алгоритмы классификации опираются на **_гипотезу компактности_**: схожим объектам соответствуют схожие ответы.

Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект *u* к тому классу *y*, для которого **суммарный вес ближайших обучающих объектов ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) максимален**:

![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%20%3A%20y_%7Bu%7D%5E%7B%28i%29%7D%20%3D%20y%7D%20w%28i%2C%20u%29%20%5Crightarrow%20max)

, где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.

Функция ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) называется **_оценкой близости объекта u к классу y_**. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.

Для поиска оптимальных параметров для каждого из рассматриваемых ниже метрических алгоритмов используется **LOO -- leave-one-out** *(критерий скользящего контроля)*, который состоит в следующем: 

1. Исключать объекты *x(i)* из выборки *Xl* по одному, получится новая выборка без объекта *x(i)* (назовём её *Xl_1*).
2. Запускать алгоритм от объекта *u*, который нужно классифицировать, на выборке *Xl_1*.
3. Завести переменную *Q* (накопитель ошибки, изначально *Q = 0*) и, когда алгоритм ошибается, *Q = Q + 1*.
4. Когда все объекты *x(i)* будут перебраны, вычислить *LOO = Q / l* (*l* -- количество объектов выборки).

При минимальном значении LOO получим оптимальный параметр алгоритма.

## Алгоритм k ближайших соседей (knn)


Для оценки близости объекта _u_ к классу _y_ алгоритм использует следующую
функцию:
![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D)
, где _i_ обозначает порядок соседа по расстоянию к точке _u_.


Алгоритм выбирает _k_ ближайших соседей и возвращает
тот класс, который среди выбранных встречается большее количество раз.

Программно алгоритм реализуется следующим образом:
```
mc.knn <- function(xl, u, k, metric=norm, sorted=FALSE){
  cols <- ncol(xl)
  rows <- nrow(xl)
  u <- unname(unlist(u))
  if(sorted != TRUE){
    umat <- matrix(rep(u, rows), rows, cols-1, byrow=TRUE)
    xl <- xl[order(metric(umat - xl[,1:(cols-1)] )),]
  }
  xl <- xl[1:k,cols]
  
  classes <- names(table(xl))
  score <- rep(0, length(classes))
  i <- 1
  
  for(el in xl){
    class <- xl[i]
    score[class] <- score[class]+1
    i <- i+1
  }

  classes[which.max(score)]
}
```

![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/1nn.png)
При k=1 получаем LOO = 0.047
А теперь посмотрим LOO при различных k
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/loo_knn.png)
Наилучший результат получаем при k=6
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/6nn.png)
Посмотрим на преимущества и недостатки

### Преимущества:
- Простота реализации.
- При *k*, подобранном около оптимального, алгоритм "неплохо" классифицирует.

### Недостатки:
- Нужно хранить всю выборку.
- При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
- При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
- Максимальная сумма объектов в *counts* может достигаться в нескольких классах одновременно.
- "Скудный" набор параметров.
- Точки, расстояние между которыми одинаково, не все будут учитываться.

## Алгоритм k взвешеных ближайших соседей (kwnn)
Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера).
Данный алгоритм классификации относит объект *u* к тому классу *y*, у которого максимальна сумма весов *w_i* его ближайших *k* соседей *x(u_i)*.

Для оценки близости классифицируемого объекта *u* к классу *y* **алгоритм wknn** использует следующую функцию:

![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D%20w%28i%29) , где *i* -- порядок соседа по расстоянию к классифицируемому объекту *u*, а *w(i)* -- строго убывающая функция веса, задаёт вклад i-го соседа в классификацию.

В приложенной программе используется весовая функция вида: ![](https://latex.codecogs.com/gif.latex?w%28i%29%20%3D%20q%5Ei%2C%20q%20%5Cepsilon%20%280%2C%201%29)

Реализация весовой функции:

``` R
mc.wlin <- function(el){
  k <- el[1]
  i <- el[2]
  (k+1-i)/k
}

```

Реализация классификатора:
``` R
mc.kwnn <- function(xl, u, k, wf, metric=norm, sorted=FALSE){
  cols <- ncol(xl)
  rows <- nrow(xl)
  u <- unname(unlist(u))
  
  if(sorted != TRUE){
    umat <- matrix(rep(u, rows), rows, cols-1, byrow=TRUE)
    w <- apply(data.frame(k, 1:k), 1, wf)
    w <- matrix(w, length(w), 1)
    xl <- xl[order(metric(umat - xl[,1:(cols-1)] )),]
  }
  xl <- xl[1:k,cols]
  
  classes <- names(table(xl))
  score <- rep(0, length(classes))
  i <- 1
  
  for(el in xl){
    class <- xl[i]
    score[class] <- score[class]+w[i]
    i <- i+1
  }
  
  classes[which.max(score)]
}
```
Давайте найдем оптимальный k в kwnn через LOO
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/loo_kwnn.png)
Получаем диапазон оптимальных k больше. 
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/6wnn.png)
Чем kwnn лучше/хуже knn?
- То же самое, что и knn
- Шире диапазон оптимальных k.
- Лучше точность на границах.

## Метод парзеновского окна
Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера). В данном алгоритме весовая функция *w_i* определяется как функция **от расстояния между классифицируемым объектом *u* и его соседями *x(u_i), i = 1, ..., l*, а не от ранга соседа *i***, как было в весовом kNN.

Для оценки близости классифицируемого объекта *u* к классу *y* **метод парзеновского окна** использует следующую функцию:

![](https://latex.codecogs.com/gif.latex?W%28i%2C%20u%29%20%3D%20K%5Cleft%20%28%20%5Cfrac%7B%5Crho%20%28u%2C%20x_%7Bu%7D%5E%7Bi%7D%29%7D%7Bh%7D%20%5Cright%20%29) , где *K(z)* -- функция ядра (не возрастающая от 0 до бесконечности), а *h* -- ширина окна (окно -- сферическая окрестность классифицируемого объекта *u* радиуса *h*).

Рассматриваются 5 ядер:
- Прямоугольное 

![](https://latex.codecogs.com/gif.latex?K%28z%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5B%7Cz%7C%20%3C%3D%201%5D)
- Треугольное 

![](https://latex.codecogs.com/gif.latex?K%28z%29%20%3D%20%281%20-%20%7Cz%7C%29%5B%7Cz%7C%20%3C%3D%201%5D)
- Квартическое 

![](https://latex.codecogs.com/gif.latex?K%28z%29%20%3D%20%5Cfrac%7B15%7D%7B16%7D*%281%20-%20z%5E%7B2%7D%29%5B%7Cz%7C%20%3C%3D%201%5D)
- Гауссовское 

![](https://latex.codecogs.com/gif.latex?K%28z%29%20%3D%20%28%282%5Cpi%29%5E%7B%5E%7B%28%5Cfrac%7B-1%7D%7B2%7D%7D%29%7D%29*e%5E%7B%28%5Cfrac%7B-z%5E2%7D%7B2%7D%29%7D)
- Епанечникова 


![](https://latex.codecogs.com/gif.latex?K%28z%29%20%3D%20%5Cfrac%7B3%7D%7B4%7D*%281%20-%20z%5E2%29%5B%7Cz%7C%20%3C%3D%201%5D)

Реализация классификатора:
``` R
mc.parzen <- function(xl, u, h, K, metric=norm){
  cols <- ncol(xl)
  rows <- nrow(xl)
  u <- unname(unlist(u))
  
  umat <- matrix(rep(u, rows), rows, cols-1, byrow=TRUE)
  distances <- metric(umat - xl[,1:(cols-1)])
  xl <- xl[,cols]
  wp <- distances/h
  
  classes <- c(names(table(xl)), "none")
  score <- rep(0, length(classes))
  score[length(classes)]<-0.00001
  i <- 1
  
  for(el in xl){
    class <- xl[i]
    score[class] <- score[class]+K(wp[i])
    i <- i+1
  }
  
  classes[which.max(score)]
}
```
Находим оптимальный шаг:
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/loo_parzen.png)
Особой разницы между ядрами в данной выборке нет. Возьмем треугольное ядро с оптимальным h = 0.4.
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/parzen.png)
Посмотрим на достоинства и недостатки:

### Плюсы
- хорошее качество классификации при правильно подобраном _h_
- все точки с одинаковым расстоянием будут учитаны

### Минусы
- необходимо хранить всю выборку целиком
- диапазон параметра _h_ необходимо подбирать самостоятельно, учитывая
плотность расположения точек
- если ни одна точка не попала в радиус _h_, алгоритм не способен ее
классифицировать (не актуально для гауссовского ядра)

## Метод парзеновского окна с переменным окном
То же самое, что и прошлый алгоритм, только каждый раз шаг подбирается в зависимости от расстояния до k соседа. 
Посмотрим LOO для различных ядер:
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/loo_parzen_auto.png)
Прямоугольное и Гауссовское ядро в начальных k дают результат лучше, чем с постоянным шагом.
В целом ничего удивительного, получаем:
- прямоугольное ядро -> knn
- треугольное ядро -> kwnn c линейным w[i]
- гауссовское ядро -> kwnn cо специальным w[i]

## Метод потенциальных функций

Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера). В данном алгоритме весовая функция *w_i* определяется как функция от расстояния между классифицируемым объектом *u* и его соседями *x(u_i), i = 1, ..., l*, как и в **методе парзеновского окна**.

Для оценки близости классифицируемого объекта *u* к классу *y* **метод потенциальных функций** использует следующую функцию:

![](https://latex.codecogs.com/gif.latex?W%28i%2C%20u%29%20%3D%20%5Cgamma_%7Bi%7D*K%5Cleft%20%28%20%5Cfrac%7B%5Crho%20%28u%2C%20x_%7Bu%7D%5E%7Bi%7D%29%7D%7Bh_i%7D%20%5Cright%20%29%2C%20%5Cgamma_%7Bi%7D%20%5Cgeqslant%200%2C%20h_i%20%3E%200) 

**Основная идея:** *Потенциалы* определяют важность каждого объекта *x_i* при классификации. Считаем, что радиусы потенциалов *h* известны заранее. Алгоритм подбирает только потенциалы ![](https://latex.codecogs.com/gif.latex?%5Cgamma_%7Bi%7D). 

Построение потенциалов:
``` R
learn.gamma <- function(f, xl, ...){
  cols <- ncol(xl)
  rows <- nrow(xl)
  g <- rep(0, rows)
  
  for(i in seq(1, rows, 1)){
    el <- xl[i,]
    class <- el[cols]
    el <- el[1:(cols-1)]
    while(class!=f(xl, el, g, ...)){
      g[i] <- g[i]+1
    }
  }
  g
}
```

Реализация алгоритма:
``` R
mc.poten <- function(xl, u, g, h, K, metric=norm){
  cols <- ncol(xl)
  rows <- nrow(xl)
  u <- unname(unlist(u))
  
  umat <- matrix(rep(u, rows), rows, cols-1, byrow=TRUE)
  distances <- metric(umat - xl[,1:(cols-1)])
  xl <- xl[,cols]
  wp <- distances/h
  
  classes <- c(names(table(xl)), "none")
  score <- rep(0, length(classes))
  score[length(classes)]<-0.00001
  i <- 1
  
  for(el in xl){
    class <- xl[i]
    score[class] <- score[class]+g[i]*K(wp[i])
    i <- i+1
  }
  
  classes[which.max(score)]
}
```

**Важное отличие алгоритма потенциальных функций от предыдущих алгоритмов:** центр "окна" располагается в обучающем объекте, а не в классифицируемом.

Карта классификации:
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/poten.png)

__Плюсы:__
- результат зависит от _2l_ параметров

__Минусы:__
- необходимо хранить всю выборку целиком
- параметры _h_ необходимо подбирать самостоятельно, алгоритм
в их подборе не принимает участия
- если ни одна точка не попала в радиус _h_, алгоритм не способен ее
классифицировать (не актуально для гауссовского ядра)
- медленно сходится
- слишком грубо настраивает параметры
- неопределенное время работы (при маленьком пороге ошибки может вообще
выполняться бесконечно)

## STOLP
В разработке...
![mc](https://raw.githubusercontent.com/bestful/ML/master/samples/stolp.png)

# Байесовские алгоритмы классификации
Байесовские алгоритмы классификации основаны на принципе максимума апостериорной вероятности : для классифицируемого объекта вычисляются плотности распределения ![](http://latex.codecogs.com/gif.latex?%5Cinline%20p%28x%7Cy%29%20%3D%20p_y%28x%29)  — **_функции правдоподобия_** классов, по ним вычисляются ***апостериорные вероятности*** - ![](http://latex.codecogs.com/gif.latex?P%20%5Cleft%20%5C%7By%7Cx%20%5Cright%20%5C%7D%20%3D%20P_yp_y%28x%29), где ![](http://latex.codecogs.com/gif.latex?%5Cinline%20P_y)- ***априорные вероятности*** классов. Объект относится к классу с максимальной апостериорной вероятностью.

*Задача классификации* - получить алгоритм ![](http://latex.codecogs.com/gif.latex?%5Cinline%20a%3A%5C%3B%20X%5Cto%20Y), способный классифицировать произвольный объект ![](http://latex.codecogs.com/gif.latex?%5Cinline%20x%20%5Cin%20X).  

1)  ***Построение классификатора при известных плотностях***  
![](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Clambda_y) - штраф за неправильное отнесение объекта класса ***??***.  
Если известны ![](http://latex.codecogs.com/gif.latex?%5Cinline%20P_y)  и ![](http://latex.codecogs.com/gif.latex?%5Cinline%20p_%7By%7D%28x%29), то минимум среднего риска ![](http://latex.codecogs.com/gif.latex?%5Cinline%20R%28a%29%20%3D%20%5Csum_%7By%5Cepsilon%20Y%7D%20%5Csum_%7Bs%5Cepsilon%20Y%7D%20%5Clambda_yP_yP%28A_s%7Cy%29), ![](http://latex.codecogs.com/gif.latex?%5Cinline%20A_s%20%3D%20%5Cbigl%5C%7Bx%20%5Cin%20X%7Ca%28x%29%3Ds%5Cbigr%5C%7D%2C)  достигается алгоритмом ![](http://latex.codecogs.com/gif.latex?%5Cinline%20a%28x%29%20%3D%20%5Carg%5Cmax%20%5Clambda_yP_yp_y%28x%29)

2) ***Восстановление плотностей по выборке***  
По подвыборке  класса *y* строим эмпирические оценки  ![](http://latex.codecogs.com/gif.latex?%5Cinline%20P_y) (доля объектов в выборке) и ![](http://latex.codecogs.com/gif.latex?%5Cinline%20p_y%28x%29).  
Три метода:  
**1)Параметрический** если плотности нормальные (гауссовские) - НДА и ЛДФ;  
**2)Непараметрический** - оценка Парзена - Розенблатта, метод парзеновского окна;   
**3)Разделение смеси** производится _ЕМ-алгоритмом_. Плотности компонент смеси (гауссовские плотности) - радиальные функции,метод радиальных базисных функций.

## Plugin
**Нормальный дискриминантный анализ** - один из вариантов байесовской классификации,где восстанавливаемые плотности - многомерные гауссовские: ![](http://latex.codecogs.com/gif.latex?%5Cinline%20N%28x%3B%5Cmu%20%2C%5Csum%29%20%3D%20%5Cfrac%7B1%7D%7B%5Csqrt%7B%282%5Cpi%29%5En%5Cleft%20%7C%20%5Csum%20%5Cright%20%7C%7D%7D%20%5Cexp%20%5Cleft%20%28%20%5Cfrac%7B-1%7D%7B2%7D%20%28x-%5Cmu%29%5ET%20%5Csum%5E%7B-1%7D%28x-%5Cmu%29%5Cright%20%29%2C%20x%20%5Cin%20%5Cmathbb%7BR%7D%5En), где ![](http://latex.codecogs.com/gif.latex?%5Cmu%20%5Cin%20%5Cmathbb%7BR%7D%5En) - мат. ожидание (центр), а ![](http://latex.codecogs.com/gif.latex?%5Csum%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20n%20%7D) - ковариационная матрица - симметричная, положительная и невырожденная.  

Находим параметры нормального распределения ![](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Cmu_y%20%3D%20%5Cfrac%20%7B1%7D%7Bl_y%7D%20%5Csum_%7Bx_i%3Ay_i%20%3D%20y%7Dx_i)**(1)**, ![](http://latex.codecogs.com/gif.latex?%5Cinline%20%5Csum_y%20%3D%20%5Cfrac%7B1%7D%7Bl_y-1%7D%20%5Csum_%7Bx_i%3Ay_i%20%3D%20y%7D%28x_i%20-%20%5Cmu_y%29%28x_i-%5Cmu_y%29%5ET)**(2)**  для ![](http://latex.codecogs.com/gif.latex?%5Cinline%20y%20%5Cin%20Y) согласно **принципу максимального правдоподобия**, подставляем в формулу ***ОБРП*** и получаем ***подстановочный алгоритм***, или ***линейный дискриминант Фишера*** (если ковариационные матрицы равны для всех классов).

[src](../PlugIn.R) 

Для сгенерированных с помощью функционала библиотеки **MASS** данных (выборка с двумя классами) вычисляем центры по **(1)**:

```R
  cols <- dim(objects)[2]
  mu <- matrix(NA, 1, cols) #создаём вектор
  for (col in 1:cols)
  {
    mu[1, col] = mean(objects[,col]) #подсчёт среднего значения для каждой из компонент
  }
```
Далее, находим ковариационные матрицы классов по **(2)** :
```R
  rows <- dim(objects)[1]
  cols <- dim(objects)[2]
  sigma <- matrix(0, cols, cols) #нулевая квадратная матрица 
  for (i in 1:rows)
  {
    sigma <- sigma + (t(objects[i,] - mu) %*% # - оператор умножения матриц, t - transpose
                        (objects[i,] - mu)) / (rows - 1)
  }
```
Из теоремы задания байесовским классификатором квадратичной поверхности получим её уравнение
![](https://latex.codecogs.com/gif.latex?%5Clambda_s%20P_sp_s%28x%29%20%3D%20%5Clambda_t%20P_tp_t%28x%29%2C)  
Логарифмируя, ![](https://latex.codecogs.com/gif.latex?%5Cln%20p_s%28x%29%20-%20%5Cln%20p_t%28x%29%20%3D%20C_%7Bst%7D),![](https://latex.codecogs.com/gif.latex?C_%7Bst%7D%20%3D%20%5Cln%28%5Clambda_tP_t/%5Clambda_sP_s%29) - константа, не зависит от x.
Осталось вычислить коэффициенты квадратичного дискриминанта из двух квадратичных форм ln p(x) : ![](https://latex.codecogs.com/gif.latex?%5Cln%20p_y%28x%29%20%3D%20-%5Cfrac%7Bn%7D%7B2%7D%5Cln2%5Cpi%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5Cln%20%7C%5Csum_y%7C-%5Cfrac%7B1%7D%7B2%7D%28x%20-%20%5Cmu_y%29%5ET%5Csum%5E%7B-1%7D_y%28x-%5Cmu_y%29)
```R
# кривая второго порядка: a*x1^2 + b*x1*x2 + c*x2 + d*x1 + e*x2 + f = 0
  invSigma1 <- solve(sigma1)#обратные матрицы
  invSigma2 <- solve(sigma2)
  f <- log(abs(det(sigma1))) - log(abs(det(sigma2))) +
    mu1 %*% invSigma1 %*% t(mu1) - mu2 %*% invSigma2 %*%
    t(mu2);
  alpha <- invSigma1 - invSigma2
  a <- alpha[1, 1] #коэффициент - элемент матрицы
  b <- 2 * alpha[1, 2]#полученный вычитанием обратных матриц
  c <- alpha[2, 2]
  beta <- invSigma1 %*% t(mu1) - invSigma2 %*% t(mu2)
  d <- -2 * beta[1, 1]#всё выводится из квадратичной формы
  e <- -2 * beta[2, 1]
  ##решаем проблему возвращения из функции набора параметров передачей их map-ом
  return (c("x^2" = a, "xy" = b, "y^2" = c, "x" = d, "y" = e, "1" = f))
```
**Результаты**

<figure>
 <img src="pics/plug1.png" width="500">
 <img src="pics/plug2.png" width="500">
 <img src="pics/NDAEllips.png" width="500">
 <img src="pics/NDA2Lines.png" width="500">
  <figcaption>Признаки некореллированы - линии уровня плотности эллипсоидные, 
    с центром mu, оси параллельны ox/oy.</figcaption>
</figure>

Классы не равновероятны или не равнозначны - разделяющая гиперплоскость отодвигается дальше от более значимого класса.  

Ковариационные матрицы не диагональны и не равны - разделяющая поверхность квадратична и прогибается - менее плотный класс охватывает более плотный.

***Недостатки***

 - Функции правдоподобия классов могут отличаться от гауссовских: когда имеются дискретные признаки, принимающие  значения, или
когда классы распадаются на изолированные участки.
 - Если длина выборки меньше размерности пространства, **?y < n**, или среди признаков есть линейно зависимые, то ковар. матрица становится вырожденной. В этом случае обратная матрица не существует и метод вообще неприменим.
 - Выборочные оценки чувствительны к нарушениям нормальности распределений, в частности, к редким большим выбросам.

![bc](https://raw.githubusercontent.com/bestful/ML/master/samples/plugin.png)

## LDF 

***Ковариационные матрицы*** классов равны, классы **s, t** равновероятны и равнозначны ![](https://latex.codecogs.com/gif.latex?%5Clambda_sP_s%20%3D%20%5Clambda_tP_t), признаки некоррелированы и имеют одинаковые ***дисперсии*** ![](https://latex.codecogs.com/gif.latex?%5Csum_s%20%3D%20%5Csum_t%20%3D%20%5Csigma%20I_n).  

Это означает, что классы имеют одинаковую сферическую форму, разделяющая плоскость проходит посередине между классами, ортогонально линии, соединяющей центры классов. Нормаль оптимальна - прямая, в одномерной проекции на которую классы разделяются наилучшим образом,с наименьшим байесовским риском **R(a)**.

<img src="pics/FLDe.png" height = "200" width="500">

Применяя ![](https://latex.codecogs.com/gif.latex?%5Cln%20p_y%28x%29%20%3D%20-%5Cfrac%7Bn%7D%7B2%7D%5Cln2%5Cpi%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5Cln%20%7C%5Csum_y%7C-%5Cfrac%7B1%7D%7B2%7D%28x%20-%20%5Cmu_y%29%5ET%5Csum%5E%7B-1%7D_y%28x-%5Cmu_y%29) , квадратичные члены сокращаются и уравнение поверхности
вырождается в линейную форму: ![](https://latex.codecogs.com/gif.latex?%28x-%5Cmu_%7Bst%7D%29%5ET%5Csum%5E%7B-1%7D%28%5Cmu_s-%5Cmu_t%29%20%3D%20C_%7Bst%7D), где ![](https://latex.codecogs.com/gif.latex?%5Cmu_%7Bst%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%28%5Cmu_s&plus;%5Cmu_t%29) - точка посередине между центрами классов.


[src](../FLD.R)

Код существенно не отличается от предыдущего алгоритма.

**Результаты**

<img src="pics/FLD1.png" width="500">

Алгоритм неплохо работает, когда формы классов действительно близки к нормальным и не слишком сильно различаются.  

В этом случае линейное решающее правило близко к оптимальному байесовскому, но устойчивее квадратичного, и часто обладает лучшей обобщающей способностью.
![bc](https://raw.githubusercontent.com/bestful/ML/master/samples/fisher.png)

![lic](https://raw.githubusercontent.com/bestful/ML/master/samples/adaline.png)